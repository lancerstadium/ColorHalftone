\documentclass[a4paper]{article}

\usepackage{fontspec}       %调用 fontspec 宏包
\usepackage{newtxtext,newtxmath} % use Times font with better support
\usepackage{xeCJK}          %调用 xeCJK 宏包
\usepackage{arxiv}
\usepackage{tikz}

% \usepackage[utf8]{inputenc} % allow utf-8 input
% \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\usepackage{xcolor}         % colors
\usepackage{listings}       % code listings
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

% 代码设置
\lstset{
    basicstyle=\small\ttfamily,                          % 设置代码字号
    breaklines=true,                                     % 自动换行
    columns=fixed,       
    numbers=left,                                        % 在左侧显示行号
    frame=none,                                          % 不显示背景边框
    backgroundcolor=\color[RGB]{245,245,244},            % 设定背景颜色
    keywordstyle=\color[RGB]{40,40,255},                 % 设定关键字颜色
    numberstyle=\scriptsize\color{darkgray},             % 设定行号格式
    commentstyle=\it\color[RGB]{0,96,96},                % 设置代码注释的格式
    stringstyle=\rmfamily\slshape\color[RGB]{128,0,0},   % 设置字符串格式
    showstringspaces=false,                              % 不显示字符串中的空格
    language=bash,                                       % 设置语言
}

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Running Title for Header}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{LOR-LUT: Low-rank Residual Lookup Table for Efficient Image Super-resolution on RISC-V-based Edge Device
%%%% Cite as
%%%% Update your official citation here when published 
\thanks{\textit{\underline{Citation}}: 
\textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Tianshuo Lu, Author2 \\
  Affiliation \\
  Univ \\
  Wuxi\\
  \texttt{\{Author1, Author2\}email@email} \\
  %% examples of more authors
   \And
  Author3 \\
  Affiliation \\
  Univ \\
  City\\
  \texttt{email@email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\maketitle


\begin{abstract}
现有的基于深度神经网络（DNN）的超分辨率（SR）方法密集的使用了卷积。其计算与存储资源的大量需求与边缘推理（Edge Inference）的机制（regime）相矛盾。最近的研究将查找表（LUT）与DNN相结合，以减少边缘推理的资源占用。然而，现有的XXXXXX。为了解决这些问题，我们通过XXX（称为WLLUT）来XXX。（细节...）。与现有的LUT方案相比，WLLUT具有优秀的推理精度和高效的硬件性能。其在五个流行的基准数据集上取得了优异的性能。代码见：。
\end{abstract}


% keywords can be removed
\keywords{Deep neural network \and look-up table \and RISC-V \and More}


\section{Introduction}
单幅图像超分辨率（SR）旨在从低分辨率（LR）图像中恢复出高分辨率（HR）图像，并在提高输入图像分辨率的同时，还原出更清晰的边缘、纹理和细节。在过去十年中，基于深度学习的 SR 方法与传统 SR 方法（如基于插值的 SR 方法、基于稀疏编码的 SR 方法）相比取得了显著的进步。然而，这类方法通常需要大量参数，计算成本较高，无法在计算资源有限的设备上实际应用。在单像超分辨率（SISR）领域，探索实用的实时 SR 解决方案已成为一个日益增长的趋势。

基于查找表 (LUT) 的超分辨率 (SR) 方法（如 SR-LUT [16]）为 LUT 中的每个潜在输入缓存经过训练的 SR 网络结果，通过用更快的索引替换运行时计算来简化推理。然而，这种策略需要有限的感受野（RF），因为输入空间大小随着输入像素的增加呈指数增长。具体来说，SR-LUT 具有 2 × 2 输入大小，通过旋转系综产生 3×3 RF，并且需要 b2×2 ×r2 字节来存储其 LUT，以便按 r 倍放大（b = 255）。例如，3 × 3 普通卷积生成 2559 个映射，以 LUT 形式消耗 1.72 TB。

更大的 RF 允许模型捕获图像中更复杂的语义和结构，在训练过程中发挥着关键作用。然而，对于 SR-LUT，LUT 尺寸的指数增长极大地限制了 RF 的改进。 MuLUT [22] 和 SPLUT [24] 等尝试解决这个问题，提出级联多个并行 LUT 将 RF 分别扩展到 9 × 9 和 6 × 6。这些方法试图将整个LUT分成几个子LUT，并且LUT大小线性增加。然而，它们并没有解决基于 LUT 的方法中 RF 尺寸受限的主要原因。此外，与 DNN SR 方法相比，这些基于 LUT 的方法的 RF 大小不足，导致性能低于简单的 DNN 模型，例如 FSRCNN[9]。

Vanilla 卷积合并了空间和通道维度上的特征，要求 LUT 格式遍历输入像素的所有潜在组合和排列。本质上，空间相关卷积是指空间邻域内的其他特征点来生成当前特征点的输入输出映射。鉴于限制基于 LUT 的方法性能的最重要因素是空间相关卷积，我们对现状提出质疑。具体来说，我们考虑将卷积过程的空间和通道计算解耦，并建议使用空间独立的卷积来存储 LUT。这可能会解决现有基于 LUT 的方法的固有局限性，为此类算法提供新的前进方向。

在本文中，我们提出了一种新的重构卷积 （RC） 方法，旨在解耦空间和通道计算，从而有效地增加 LUT 的 RF，同时显著降低存储要求。这种解耦操作使网络能够规避遍历所有空间像素组合的必要性所带来的约束。因此，我们的方法可以采用 n × n 个 1D LUT 来近似 n×n卷积层的效果，将 LUT 大小从初始 bn2 切割到 b×n2。基于我们的 RC方法，我们引入了一种为 SR 任务量身定制的实用的基于重构卷积模块的 LUT 方法 （RCLUT），允许以最小的存储消耗扩展 RF。如图 1 所示，我们的 RCLUT 以最小的 LUT 大小获得了有竞争力的性能，展示了性能和 LUT 存储之间的最佳平衡。


此外，我们的 RC 方法可以有效地集成为插件模块。尽管 RC 模块破坏了二维特征之间的交互信息，但其最小的存储要求和较大的接收场成功地抵消了以前基于 LUT 的方法的缺点。在我们的实施中，我们将 RC 模块集成在 SRLUT 之上，因此成本仅为原始存储的 1/10,000 左右，提高了 13 倍 RF 大小。



总而言之，我们工作的贡献包括： 
\begin{itemize}
\item 我们提出了一种新颖的具有大RF的重构卷积（RC）方法，该方法解耦了卷积的空间和通道计算。基于 RC 方法，我们的 RCLUT 模型以更少的存储实现了显着的性能。 
\item 我们的RC 方法可以设计为插件模块，这可以改进基于LUT 的SR 方法，但尺寸略有增加。 
\item 大量结果表明，与基于LUT 的SR 方法相比，我们的方法获得了优越的性能。它是 SR 任务中一种新的最先进的 LUT 方法。
\end{itemize}


\begin{lstlisting}
    # 1、安装latexdiff
    git latexdiff --xelatex --quiet --main demo.tex 8486ea3 cc306a6
\end{lstlisting}



\section{Related Work}
\subsection{Development of Super-Resolution}
近年来，人们对边缘设备上高质量 SR 的追求激发了对高效 SISR 方案的兴趣和研究，这些方案大致分为传统方法、深度学习方法和基于 LUT 的方法。

传统的基于插值的超分辨率方法，如：Nearest, Bilinear 和 Bicubic，虽然效率很高，但由于忽略了图像细节，结果往往比较模糊。基于稀疏编码的方法通过学习到稀疏字典将低分辨率图像复原成高分辨率图像。经过运行成本大幅增加，但其结果优于基于插值的方法。

基于深度学习的 SR 方法取得了巨大进步，SRCNN、VDSR、EDSR 和 RCAN 等架构都以提高性能为目标。然而，这些方法需要大量的计算资源。为了缓解这一问题，ESPCN 和 FSRCNN 采用了更小的网络。其他策略包括减少参数、稀疏化或量化，在保持性能的同时减少计算需求。不过，对于计算资源有限的设备（如移动设备）来说，这些模型仍然过于繁重。

随着边缘推理的需求日益增加，越来越多的研究关注基于 LUT 的方法。Jo 和 Kim 提出了一种在 SR 中使用 LUT 的方法（SR-LUT）\cite{jo2021practical}。他们训练了一个具有感受野（RF）的简单深度 SR 网络，并将输入（作为索引）和网络输出（作为索引的位置和作为值的像素强度）传输到 LUT。LUT 用于在扫描过程中生成最终结果。由于不需要额外的计算，SR-LUT 在移动设备上的运行速度与基于插值法的 SR 方法一样快，即几十毫秒。由于 LUT 的大小与索引容量成指数增长，因此 SR-LUT 中的 RF 限制为 3 × 3。然而，文献证明 RF 的大小是至关重要的，SR LUT 自然会获得较差的性能。Li 等人提出了 MuLUT，通过引入手工制作的索引模式和级联 LUT 来增加 RF。通过这两种方案，MuLUT 与 SR-LUT 相比取得了显著的改进，但代价是 LUT 总大小的线性增长。同样，Ma 等人也采用级联 LUT 的思路来扩大 RF。他们提出了串并联 LUT (SPLUT) 框架，该框架引入了通道级 LUT，并并行处理从原始 8 位输入中分离出来的两个分量。这两种方法将 SR-LUT 的单 LUT 转换为多 LUT，从而提高了 SR 模型的 RF。


\subsection{Development of Low-rank Decomposition}
人们发现深度学习模型的权重矩阵在参数数量上存在冗余。随后，出现了一系列研究致力于寻找权重张量的有效秩，并通过低秩张量逼近原始张量来减少参数数量。


\section{Method1}
\subsection{Group Separable Mapping Strategy}

\section{Method2}
\subsection{Figures}
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11] 

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
  \label{fig:fig1}
\end{figure}


\subsection{Headings: second level}
\lipsum[5]
\begin{equation}
\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
\end{equation}


\section{Evaluation}
在本节中，我们首先介绍了数据集和我们提出的 RCLUT 网络的训练细节。随后，我们通过定量和定性评估，将 RCLUT 与几种基于 LUT 的先进 SR 方法进行了比较。此外，我们还量化了将我们的 RC-Plugin 模块纳入其他基于 LUT 方法时的优势。最后，我们进行了消融研究，以展示我们的 RCLUT 模型和 RC 模块的有效性。

\subsection{Experimental Setting}

\paragraph{数据集和指标} 我们在 DIV2K 数据集上训练 RCLUT 网络，该数据集包含 2K 分辨率的 SR 任务图像。我们利用 DIV2K 中广泛使用的 800 幅训练图像来训练 RCLUT 模型。我们的重点是 SR 任务中的 ×4 放大系数，低分辨率 (LR) 图像只需通过双三次插值降频即可。我们使用 Set5、Set14、BSD100、Urban100 和 Manga109 等公开基准来评估我们方法的性能。为了公平比较，我们仅使用 Y 通道的峰值信噪比（PSNR）和结构相似性指数（SSIM） 作为评估指标。此外，我们还考虑了 LUT 的存储大小，以评估其他方法的效率。

\paragraph{训练设置} 我们的训练设置如下：我们将 RC 线性运算中的信道增量 C 设为 64。使用 Adam 优化器[19]对 RCLUT 模型进行 200,000 次迭代训练，学习率为 1e-4，批量大小为 32。选择均方误差（MSE）损失函数作为优化目标。我们还采用了轮换训练策略，以提高成绩和射频。我们还采用了旋转训练策略来提高性能和射频。随机翻转和旋转等数据增强技术用于提高我们模型的能力。我们在 Nvidia V100 GPU 上使用 PyTorch 训练 RCLUT 模型。

\paragraph{缓存 LUT 设置} 当模型收敛后，我们将 RCLUT 网络转换为间隔为 24 的多 LUT 格式，以减小体积。由于采用了级联结构，第一阶段的结果需要量化到 tgers 中，因此我们采用了与 MuLUT 相同的重新索引方法。此外，为了确保 LUT 的性能与网络的性能一致，我们还采用了 MuLUT 提出的 LUT 感知微调策略。



\subsection{Quantitative Evaluation}
在本节中，我们将我们的方法与各种 SR 方法进行了比较，包括 3 种基于插值的方法（最近邻插值、双线性插值和双三次插值）、4 种稀疏编码方法（NE+LLE[5]、Zeydeetal[40]、ANR[33]和 A+[34]）、1 种基于 DNN 的方法（FSRCNN[9]）和 3 种基于 LUT 的方法（SR-LUT[16]、MuLUT[22]和 SPLUT[24] ）。

\subsection{Qualitative Evaluation}

在本节中，我们主要将我们的方法与 3 种基于 LUT 的 SR 方法和 FSRCNN 网络进行比较。图 5 说明了基准数据集中 5 个案例的视觉质量，补充材料中提供了额外的视觉结果。如第一个和最后一个示例所示，双三次插值导致输出模糊，SR LUT 由于有限的 RF 大小而引入了明显的阻塞伪影，FSRCNN 产生严重的棋盘格效应，MuLUT 和 SPLUT 的质量比 SR-LUT 相对较好，但仍包含噪声。相比之下，我们的方法获得了更令人满意的结果，说明了更清晰的边缘和更少的伪影。其他三个示例表明，即使与 DNN 方法 （FSRCNN） 相比，我们的 RCLUT 模型也能恢复更清晰的边缘和更自然的纹理。这些渐进式视觉结果验证了 RCLUT 在利用扩展 RF 尺寸方面的有效性。


\subsection{Ablation Studies}
在本节中，我们将讨论 RCLUT 网络的有效性和 RC 模块的实现。


\begin{table}
    \caption{Comparison with other methods}
    \centering
    \begin{tabular}{ccccccccc}
        \toprule
        & Method    & Model Size    & LUT Size  & Set5  & Set14 & BSDS100   & Urban100  & Manga109 \\
        \midrule
        & Nearest   & -             & -      \\
        Classic  & Bilinear   & -             & -      \\
        & Bicubic   & -             & -      \\
        \midrule
        DNN      & Input terminal  & $\sim$100     \\
        \midrule
        LUT      & Output terminal & $\sim$10      \\
        \bottomrule
    \end{tabular}
    \label{tab:tab_01}
\end{table}

\section{Conclusion}
在本文中，我们提出了一种重构卷积法，它将空间和信道特征的计算分离开来。RC 方法可被表述为多一维 LUT，从而以较小的 LUT 大小保持较大的射频。此外，我们还提出了一种基于 RC 块的 RCLUT 模型。它在 LUT 尺寸略有减小的情况下实现了显著的性能。另一方面，我们的 RC 方法可用作插件模块，以提高 LUT 能力的方法。广泛的实验表明，我们的 RCLUT 是目前最先进的用于 SR 任务的 LUT 方法，它的工作效率远远高于其他优先方法。


\section*{Acknowledgments}
This was was supported in part by......

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}
